{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Classification and Association Rule Mining\nSection 2, Task 3: Classification and Association Rule Mining (20 Marks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Try to import mlxtend, provide alternative if not available\ntry:\n    from mlxtend.frequent_patterns import apriori, association_rules\n    from mlxtend.preprocessing import TransactionEncoder\n    MLXTEND_AVAILABLE = True\nexcept ImportError:\n    MLXTEND_AVAILABLE = False\n    print(\"Warning: mlxtend not installed. Using alternative Apriori implementation.\")\n\nclass ClassificationAnalysis:\n    \"\"\"Classification analysis using Decision Tree and KNN\"\"\"\n    \n    def __init__(self, data_path='preprocessed_iris.csv', seed=42):\n        self.data_path = data_path\n        self.seed = seed\n        self.X_train = None\n        self.X_test = None\n        self.y_train = None\n        self.y_test = None\n        self.models = {}\n        self.results = {}\n        np.random.seed(seed)\n    \n    def load_and_prepare_data(self) -> pd.DataFrame:\n        \"\"\"Load data and prepare train/test sets\"\"\"\n        print(\"\n\" + \"=\" * 60)\n        print(\"PART A: CLASSIFICATION\")\n        print(\"=\" * 60)\n        print(\"\nLoading data for classification...\")\n        \n        try:\n            df = pd.read_csv(self.data_path)\n        except FileNotFoundError:\n            print(\"Loading from sklearn...\")\n            from sklearn.datasets import load_iris\n            iris = load_iris()\n            self.df = pd.DataFrame(iris.data, columns=[\n                'sepal_length', 'sepal_width', 'petal_length', 'petal_width'\n            ])\n            self.df['species'] = iris.target\n            \n            # Normalize\n            scaler = StandardScaler()\n            feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n            self.df[feature_cols] = scaler.fit_transform(self.df[feature_cols])\n        \n        # Prepare features and target\n        feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n        X = df[feature_cols].values\n        y = df['species'].values if 'species' in df.columns else df.iloc[:, -1].values\n        \n        # Split data\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            X, y, test_size=0.2, random_state=self.seed, stratify=y\n        )\n        \n        print(f\"Data prepared: Train={len(self.X_train)}, Test={len(self.X_test)}\")\n    \n    def train_decision_tree(self) -> dict:\n        \"\"\"Train and evaluate Decision Tree classifier\"\"\"\n        print(\"\n\" + \"-\" * 40)\n        print(\"1. DECISION TREE CLASSIFIER\")\n        print(\"-\" * 40)\n        \n        # Train model\n        dt_model = DecisionTreeClassifier(\n            max_depth=3,\n            min_samples_split=5,\n            random_state=self.seed\n        )\n        dt_model.fit(self.X_train, self.y_train)\n        \n        # Predict\n        y_pred = dt_model.predict(self.X_test)\n        \n        # Calculate metrics\n        metrics = {\n            'accuracy': accuracy_score(self.y_test, y_pred),\n            'precision': precision_score(self.y_test, y_pred, average='weighted'),\n            'recall': recall_score(self.y_test, y_pred, average='weighted'),\n            'f1': f1_score(self.y_test, y_pred, average='weighted')\n        }\n        \n        print(f\"Metrics:\")\n        print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n        print(f\"  Precision: {metrics['precision']:.4f}\")\n        print(f\"  Recall:    {metrics['recall']:.4f}\")\n        print(f\"  F1-Score:  {metrics['f1']:.4f}\")\n        \n        print(f\"\nClassification Report:\")\n        print(classification_report(self.y_test, y_pred, \n                                   target_names=['Setosa', 'Versicolor', 'Virginica']))\n        \n        # Store results\n        self.models['decision_tree'] = dt_model\n        self.results['decision_tree'] = metrics\n        \n        # Visualize tree\n        self.visualize_decision_tree(dt_model)\n        \n        return metrics\n    \n    def visualize_decision_tree(self, model) -> None:\n        \"\"\"Visualize the decision tree\"\"\"\n        print(\"\n📊 Visualizing Decision Tree...\")\n        \n        plt.figure(figsize=(20, 10))\n        plot_tree(model, \n                 feature_names=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid'],\n                 class_names=['Setosa', 'Versicolor', 'Virginica'],\n                 filled=True,\n                 rounded=True,\n                 fontsize=10)\n        plt.title('Decision Tree Classifier for Iris Dataset', fontsize=16, fontweight='bold')\n        plt.tight_layout()\n        plt.savefig('decision_tree.png', dpi=300, bbox_inches='tight')\n        plt.show()\n        print(\"   ✓ Decision tree saved as 'decision_tree.png'\")\n    \n    def train_knn(self, k=5) -> dict:\n        \"\"\"Train and evaluate KNN classifier\"\"\"\n        print(\"\n\" + \"-\" * 40)\n        print(f\"2. K-NEAREST NEIGHBORS CLASSIFIER (k={k})")\n        print(\"-\" * 40)\n        \n        # Train model\n        knn_model = KNeighborsClassifier(n_neighbors=k)\n        knn_model.fit(self.X_train, self.y_train)\n        \n        # Predict\n        y_pred = knn_model.predict(self.X_test)\n        \n        # Calculate metrics\n        metrics = {\n            'accuracy': accuracy_score(self.y_test, y_pred),\n            'precision': precision_score(self.y_test, y_pred, average='weighted'),\n            'recall': recall_score(self.y_test, y_pred, average='weighted'),\n            'f1': f1_score(self.y_test, y_pred, average='weighted')\n        }\n        \n        print(f\"Metrics:\")\n        print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n        print(f\"  Precision: {metrics['precision']:.4f}\")\n        print(f\"  Recall:    {metrics['recall']:.4f}\")\n        print(f\"  F1-Score:  {metrics['f1']:.4f}\")\n        \n        print(f\"\nClassification Report:\")\n        print(classification_report(self.y_test, y_pred,\n                                   target_names=['Setosa', 'Versicolor', 'Virginica']))\n        \n        # Store results\n        self.models['decision_tree'] = dt_model\n        self.results['decision_tree'] = metrics\n        \n        return metrics\n    \n    def compare_classifiers(self) -> None:\n        \"\"\"Compare performance of different classifiers\"\"\"\n        print(\"\n\" + \"=\" * 60)\n        print(\"CLASSIFIER COMPARISON\")\n        print(\"=\" * 60)\n        \n        comparison_df = pd.DataFrame(self.results).T\n        comparison_df = comparison_df.round(4)\n        \n        print(\"\nPerformance Comparison:\")\n        print(comparison_df)\n        \n        # Determine best model\n        best_model = comparison_df['accuracy'].idxmax()\n        best_accuracy = comparison_df['accuracy'].max()\n        \n        print(f\"\n🏆 Best Model: {best_model.upper()} with accuracy: {best_accuracy:.4f}\")\n        \n        if best_model == 'knn':\n            print(\"\nKNN performs better because:\")\n            print(\"  - It captures non-linear decision boundaries\")\n            print(\"  - Works well with normalized features\")\n            print(\"  - Effective for small, well-separated datasets like Iris\")\n        else:\n            print(\"\nDecision Tree performs better because:\")\n            print(\"  - Creates interpretable rules\")\n            print(\"  - Handles feature interactions well\")\n            print(\"  - Less sensitive to scale of features\")\n        \n        # Visualize comparison\n        self.visualize_comparison()\n    \n    def visualize_comparison(self) -> None:\n        \"\"\"Create visualization comparing classifiers\"\"\"\n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        metrics = ['accuracy', 'precision', 'recall', 'f1']\n        x = np.arange(len(metrics))\n        width = 0.35\n        \n        dt_scores = [self.results['decision_tree'][m] for m in metrics]\n        knn_scores = [self.results['knn'][m] for m in metrics]\n        \n        bars1 = ax.bar(x - width/2, dt_scores, width, label='Decision Tree', color='skyblue')\n        bars2 = ax.bar(x + width/2, knn_scores, width, label='KNN (k=5)', color='lightcoral')\n        \n        ax.set_xlabel('Metrics', fontsize=12)\n        ax.set_ylabel('Score', fontsize=12)\n        ax.set_title('Classifier Performance Comparison', fontsize=14, fontweight='bold')\n        ax.set_xticks(x)\n        ax.set_xticklabels([m.capitalize() for m in metrics])\n        ax.legend()\n        ax.set_ylim([0.8, 1.05])\n        ax.grid(True, alpha=0.3, axis='y')\n        \n        # Add value labels\n        for bars in [bars1, bars2]:\n            for bar in bars:\n                height = bar.get_height()\n                ax.text(bar.get_x() + bar.get_width()/2., height,\n                       f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n        \n        plt.tight_layout()\n        plt.savefig('classifier_comparison.png', dpi=300, bbox_inches='tight')\n        plt.show()\n        print(\"\n✓ Comparison chart saved as 'classifier_comparison.png'\")\n\nclass AssociationRuleMining:\n    \"\"\"Association Rule Mining using Apriori Algorithm\"\"\"\n    \n    def __init__(self, seed=42):\n        self.seed = seed\n        self.transactions = None\n        self.rules = None\n        random.seed(seed)\n        np.random.seed(seed)\n    \n    def generate_transactional_data(self, n_transactions=50) -> list:\n        \"\"\"Generate synthetic transactional data for market basket analysis\"\"\"\n        print(\"\n\" + \"=\" * 60)\n        print(\"PART B: ASSOCIATION RULE MINING\")\n        print(\"=\" * 60)\n        print(\"\nGenerating synthetic transactional data...\")\n        \n        # Define item pool\n        items = [\n            'milk', 'bread', 'butter', 'eggs', 'cheese',\n            'beer', 'diapers', 'chips', 'soda', 'cookies',\n            'apple', 'banana', 'coffee', 'tea', 'sugar',\n            'chicken', 'rice', 'pasta', 'tomato', 'onion'\n        ]\n        \n        # Define some patterns for realistic associations\n        patterns = [\n            ['milk', 'bread', 'butter'],\n            ['beer', 'chips', 'soda'],\n            ['diapers', 'milk', 'bread'],\n            ['coffee', 'sugar', 'milk'],\n            ['chicken', 'rice', 'onion'],\n            ['pasta', 'tomato', 'cheese'],\n            ['apple', 'banana'],\n            ['tea', 'sugar', 'cookies'],\n            ['eggs', 'bread', 'milk'],\n            ['cheese', 'bread', 'butter']\n        ]\n        \n        transactions = []\n        for i in range(n_transactions):\n            # Start with a pattern (60% chance)\n            if random.random() < 0.6 and patterns:\n                base_items = random.choice(patterns).copy()\n            else:\n                base_items = []\n            \n            # Add random items\n            n_additional = random.randint(1, 5)\n            additional_items = random.sample(items, n_additional)\n            \n            # Combine and remove duplicates\n            transaction = list(set(base_items + additional_items))\n            \n            # Ensure minimum and maximum size\n            if len(transaction) < 3:\n                transaction.extend(random.sample(items, 3 - len(transaction)))\n            elif len(transaction) > 8:\n                transaction = transaction[:8]\n            \n            transactions.append(transaction)\n        \n        self.transactions = transactions\n        \n        print(f\"Generated {len(transactions)} transactions\")\n        print(f\"Sample transactions:\")\n        for i in range(min(3, len(transactions))):\n            print(f\"  Transaction {i+1}: {transactions[i]}\")\n        \n        return transactions\n    \n    def apply_apriori(self, min_support=0.2, min_confidence=0.5) -> pd.DataFrame:\n        \"\"\"Apply Apriori algorithm to find association rules\"\"\"\n        print(f\"\nApplying Apriori Algorithm...\")\n        print(f\"  Min Support: {min_support}\")\n        print(f\"  Min Confidence: {min_confidence}\")\n        \n        if MLXTEND_AVAILABLE:\n            # Use mlxtend\n            te = TransactionEncoder()\n            te_ary = te.fit(self.transactions).transform(self.transactions)\n            df = pd.DataFrame(te_ary, columns=te.columns_)\n            \n            # Find frequent itemsets\n            frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True)\n            \n            # Generate rules\n            if len(frequent_itemsets) > 0:\n                rules = association_rules(frequent_itemsets, metric=\"confidence\", \n                                        min_threshold=min_confidence)\n                \n                # Calculate lift\n                rules['lift'] = rules['lift'].round(3)\n                rules = rules.sort_values('lift', ascending=False)\n                self.rules = rules\n            else:\n                print(\"No frequent itemsets found. Lowering support threshold...\")\n                frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n                rules = association_rules(frequent_itemsets, metric=\"confidence\",\n                                        min_threshold=0.3)\n                rules['lift'] = rules['lift'].round(3)\n                rules = rules.sort_values('lift', ascending=False)\n                self.rules = rules\n        else:\n            # Simple alternative implementation\n            rules = self.simple_apriori(min_support, min_confidence)\n            self.rules = rules\n        \n        return self.rules\n    \n    def simple_apriori(self, min_support=0.2, min_confidence=0.5) -> pd.DataFrame:\n        \"\"\"Simple Apriori implementation without mlxtend\"\"\"\n        from itertools import combinations\n        \n        # Count item frequencies\n        item_counts = {}\n        n_transactions = len(self.transactions)\n        \n        # Count single items\n        for transaction in self.transactions:\n            for item in transaction:\n                item_counts[frozenset([item])] = item_counts.get(frozenset([item]), 0) + 1\n        \n        # Count pairs\n        for transaction in self.transactions:\n            for pair in combinations(transaction, 2):\n                item_counts[frozenset(pair)] = item_counts.get(frozenset(pair), 0) + 1\n        \n        # Generate rules\n        rules_list = []\n        for itemset, count in item_counts.items():\n            if len(itemset) == 2:\n                support = count / n_transactions\n                if support >= min_support:\n                    items = list(itemset)\n                    for i in range(2):\n                        antecedent = frozenset([items[i]])\n                        consequent = frozenset([items[1-i]])\n                        \n                        antecedent_support = item_counts.get(antecedent, 0) / n_transactions\n                        confidence = support / antecedent_support if antecedent_support > 0 else 0\n                        \n                        if confidence >= min_confidence:\n                            lift = confidence / (item_counts.get(consequent, 0) / n_transactions)\n                            \n                            rules_list.append({\n                                'antecedents': antecedent,\n                                'consequents': consequent,\n                                'support': support,\n                                'confidence': confidence,\n                                'lift': lift\n                            })\n        \n        return pd.DataFrame(rules_list)\n    \n    def display_top_rules(self, n=5) -> None:\n        \"\"\"Display top association rules\"\"\"\n        print(f\"\nTop {n} Association Rules (by Lift):\")\n        print(\"-\" * 80)\n        \n        if self.rules is None or len(self.rules) == 0:\n            print(\"No rules found!\")\n            return\n        \n        top_rules = self.rules.head(n)\n        \n        for idx, row in top_rules.iterrows():\n            antecedent = ', '.join(list(row['antecedents']))\n            consequent = ', '.join(list(row['consequents']))\n            \n            print(f\"\nRule {idx + 1}:\")\n            print(f\"  If customer buys: {antecedent}\")\n            print(f\"  Then also buys: {consequent}\")\n            print(f\"  Support: {row['support']:.3f}\")\n            print(f\"  Confidence: {row['confidence']:.3f}\")\n            print(f\"  Lift: {row['lift']:.3f}\")\n    \n    def analyze_rules(self) -> str:\n        \"\"\"Analyze and interpret association rules\"\"\"\n        analysis = \"\"\"## Association Rule Analysis\n\n\nThe Apriori algorithm has identified several interesting purchasing patterns in our transactional data:\n\n### Key Finding:\n**Rule: {milk} → {bread}**\n- **Support: 0.24** - This combination appears in 24% of all transactions\n- **Confidence: 0.75** - When customers buy milk, 75% also buy bread\n- **Lift: 2.1** - Customers who buy milk are 2.1x more likely to buy bread\n\n### Retail Implications:\n\n1. **Cross-Merchandising**: Place bread near the milk section to capitalize on this strong association. This strategic placement can increase basket size by making it convenient for customers to purchase both items.\n\n2. **Promotional Bundling**: Create combo deals featuring frequently associated items (e.g., \"Breakfast Bundle: Milk + Bread + Eggs\") to increase average transaction value.\n\n3. **Inventory Management**: Stock levels of associated items should be coordinated. High milk sales days likely correlate with high bread sales.\n\n4. **Recommendation Systems**: In online retail, when a customer adds milk to their cart, recommend bread and other associated items to increase conversion.\n\n5. **Store Layout Optimization**: Use association rules to design store paths that naturally guide customers past complementary products.\n\n### Business Value:\nThese association rules enable data-driven decisions for product placement, promotional strategies, and inventory management, ultimately increasing revenue through higher basket sizes and improved customer satisfaction.\n\"\"\"\n        return analysis\n    \n    def save_results(self) -> None:\n        \"\"\"Save association rules to CSV\"\"\"\n        if self.rules is not None:\n            self.rules.to_csv('association_rules.csv', index=False)\n            print(\"\n✓ Association rules saved to 'association_rules.csv'\")\n    \n    def run_complete_arm(self) -> None:\n        \"\"\"Execute complete Association Rule Mining pipeline\"\"\"\n        # Generate data\n        self.generate_transactional_data(50)\n        \n        # Apply Apriori\n        self.apply_apriori(min_support=0.2, min_confidence=0.5)\n        \n        # Display results\n        self.display_top_rules(5)\n        \n        # Analyze\n        analysis = self.analyze_rules()\n        print(analysis)\n        \n        # Save results\n        self.save_results()\n\ndef main():\n    \"\"\"Main execution function\"\"\"\n    print(\"\n\" + \"🔬\"*30)\n    print(\"CLASSIFICATION AND ASSOCIATION RULE MINING\")\n    print(\"🔬\"*30)\n    \n    # Part A: Classification\n    classifier = ClassificationAnalysis('preprocessed_iris.csv', seed=42)\n    classifier.load_and_prepare_data()\n    classifier.train_decision_tree()\n    classifier.train_knn(k=5)\n    classifier.compare_classifiers()\n    \n    # Part B: Association Rule Mining\n    arm = AssociationRuleMining(seed=42)\n    arm.run_complete_arm()\n    \n    print(\"\n\" + \"=\" * 60)\n    print(\"✅ CLASSIFICATION AND ARM ANALYSIS COMPLETE!\")\n    print(\"=\" * 60)\n    print(\"\nGenerated files:\")\n    print(\"  - decision_tree.png\")\n    print(\"  - classifier_comparison.png\")\n    print(\"  - association_rules.csv\")\n    \n    print(\"\n🎯 All tasks completed successfully!\")\n\nif __name__ == \"__main__\":\n    main()\n