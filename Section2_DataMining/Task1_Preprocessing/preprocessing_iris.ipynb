{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Data Preprocessing and Exploration for Iris Dataset\nSection 2, Task 1: Data Preprocessing and Exploration (15 Marks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass IrisPreprocessor:\n    \"\"\"Preprocessing and Exploration for Iris Dataset\"\"\"    \n    def __init__(self, use_synthetic=False, seed=42):\n        self.seed = seed\n        self.use_synthetic = use_synthetic\n        self.df = None\n        self.X_train = None\n        self.X_test = None\n        self.y_train = None\n        self.y_test = None\n        np.random.seed(seed)\n        \n    def generate_synthetic_iris(self) -> pd.DataFrame:\n        \"\"\"Generate synthetic data mimicking Iris dataset\"\"\"\n        print(\"Generating synthetic Iris-like dataset...")\n        \n        np.random.seed(self.seed)\n        \n        # Generate 150 samples (50 per class) with 4 features\n        n_samples_per_class = 50\n        \n        # Class 0 (Setosa-like): smaller measurements\n        class_0 = np.random.normal(loc=[5.0, 3.4, 1.5, 0.2], \n                                  scale=[0.35, 0.38, 0.17, 0.10], \n                                  size=(n_samples_per_class, 4))\n        \n        # Class 1 (Versicolor-like): medium measurements\n        class_1 = np.random.normal(loc=[5.9, 2.8, 4.3, 1.3], \n                                  scale=[0.51, 0.31, 0.47, 0.20], \n                                  size=(n_samples_per_class, 4))\n        \n        # Class 2 (Virginica-like): larger measurements\n        class_2 = np.random.normal(loc=[6.5, 3.0, 5.5, 2.0], \n                                  scale=[0.63, 0.32, 0.55, 0.27], \n                                  size=(n_samples_per_class, 4))\n        \n        # Combine all classes\n        X = np.vstack([class_0, class_1, class_2])\n        y = np.array([0]*n_samples_per_class + [1]*n_samples_per_class + [2]*n_samples_per_class)\n        \n        # Create DataFrame\n        df = pd.DataFrame(X, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n        df['species'] = y\n        df['species_name'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n        \n        # Add some missing values for demonstration (2% of data)\n        n_missing = int(0.02 * len(df) * 4)\n        for _ in range(n_missing):\n            row = np.random.randint(0, len(df))\n            col = np.random.choice(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n            df.loc[row, col] = np.nan\n        \n        print(f"Generated {len(df)} samples with {df.isnull().sum().sum()} missing values")\n        \n        return df\n    \n    def load_data(self) -> pd.DataFrame:\n        \"\"\"Load Iris dataset or generate synthetic data\"\"\"\n        print("\n" + "="*60)\n        print("STEP 1: LOADING DATA")\n        print("="*60)\n        \n        if self.use_synthetic:\n            self.df = self.generate_synthetic_iris()\n        else:\n            # Load from sklearn\n            iris = load_iris()\n            self.df = pd.DataFrame(iris.data, columns=iris.feature_names)\n            self.df['species'] = iris.target\n            self.df['species_name'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n            \n            # Clean column names\n            self.df.columns = [col.replace(' (cm)', '').replace(' ', '_') for col in self.df.columns]\n        \n        print(f"Dataset loaded successfully!")\n        print(f"Shape: {self.df.shape}")\n        print(f"Columns: {list(self.df.columns)}")\n        \n        return self.df\n    \n    def preprocess_data(self) -> pd.DataFrame:\n        \"\"\"Preprocess the data: handle missing values, normalize, encode\"\"\"\n        print("\n" + "="*60)\n        print("STEP 2: PREPROCESSING")\n        print("="*60)\n        \n        # 1. Check for missing values\n        print("\n1. Checking for missing values:")\n        missing = self.df.isnull().sum()\n        print(missing[missing > 0] if missing.sum() > 0 else "No missing values found")\n        \n        # 2. Handle missing values (if any)\n        if self.df.isnull().sum().sum() > 0:\n            print("\n2. Handling missing values with mean imputation...")\n            numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n            for col in numeric_cols:\n                if self.df[col].isnull().sum() > 0:\n                    mean_val = self.df[col].mean()\n                    self.df[col].fillna(mean_val, inplace=True)\n                    print(f"   Filled {col} with mean: {mean_val:.2f}")\n        \n        # 3. Normalize features using Min-Max scaling\n        print("\n3. Normalizing features using Min-Max scaling...")\n        scaler = MinMaxScaler()\n        feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n        self.df[feature_cols] = scaler.fit_transform(self.df[feature_cols])\n        print("   Features normalized to range [0, 1]")\n        \n        # 4. Encode the class label\n        print("\n4. Encoding class labels...")\n        le = LabelEncoder()\n        self.df['species_encoded'] = le.fit_transform(self.df['species_name'])\n        print(f"   Classes encoded: {dict(zip(le.classes_, le.transform(le.classes_)))}")\n        \n        return self.df\n    \n    def explore_data(self) -> None:\n        \"\"\"Explore data with statistics and visualizations\"\"\"\n        print("\n" + "="*60)\n        print("STEP 3: DATA EXPLORATION")\n        print("="*60)\n        \n        # 1. Summary statistics\n        print("\n1. Summary Statistics:")\n        print(self.df.describe().round(3))\n        \n        # 2. Class distribution\n        print("\n2. Class Distribution:")\n        print(self.df['species_name'].value_counts())\n        \n        # 3. Correlation matrix\n        print("\n3. Correlation Matrix:")\n        feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n        corr_matrix = self.df[feature_cols].corr()\n        print(corr_matrix.round(3))\n        \n        # Create visualizations\n        self.create_visualizations()\n    \n    def create_visualizations(self) -> None:\n        \"\"\"Create and save visualization plots\"\"\"\n        print("\n4. Creating Visualizations...")\n        \n        feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n        \n        # Set style\n        sns.set_style("whitegrid")\n        \n        # 1. Pairplot\n        fig = plt.figure(figsize=(12, 10))\n        pairplot_data = self.df[feature_cols + ['species_name']].copy()\n        g = sns.pairplot(pairplot_data, hue='species_name', palette='Set1', diag_kind='kde')\n        g.fig.suptitle('Iris Dataset - Pairplot', y=1.02, fontsize=16, fontweight='bold')\n        plt.tight_layout()\n        plt.savefig('iris_pairplot.png', dpi=300, bbox_inches='tight')\n        plt.show()\n        print("   ✓ Pairplot saved as 'iris_pairplot.png'")\n        \n        # 2. Correlation Heatmap\n        plt.figure(figsize=(8, 6))\n        corr_matrix = self.df[feature_cols].corr()\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n                   square=True, linewidths=1, cbar_kws={"shrink": 0.8})\n        plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n        plt.tight_layout()\n        plt.savefig('iris_correlation.png', dpi=300, bbox_inches='tight')\n        plt.show()\n        print("   ✓ Correlation heatmap saved as 'iris_correlation.png'")\n        \n        # 3. Boxplots for outlier detection\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n        for idx, col in enumerate(feature_cols):\n            ax = axes[idx // 2, idx % 2]\n            self.df.boxplot(column=col, by='species_name', ax=ax)\n            ax.set_title(f'{col.replace("_", " ").title()} by Species')\n            ax.set_xlabel('Species')\n            ax.set_ylabel(col.replace("_", " ").title())\n            ax.get_figure().suptitle('')\n        \n        plt.suptitle('Boxplots for Outlier Detection', fontsize=16, fontweight='bold')\n        plt.tight_layout()\n        plt.savefig('iris_boxplots.png', dpi=300, bbox_inches='tight')\n        plt.show()\n        print("   ✓ Boxplots saved as 'iris_boxplots.png'")\n        \n        # Identify outliers\n        print("\n5. Outlier Detection (using IQR method):")\n        for col in feature_cols:\n            Q1 = self.df[col].quantile(0.25)\n            Q3 = self.df[col].quantile(0.75)\n            IQR = Q3 - Q1\n            outliers = self.df[(self.df[col] < Q1 - 1.5*IQR) | (self.df[col] > Q3 + 1.5*IQR)]\n            if len(outliers) > 0:\n                print(f"   {col}: {len(outliers)} outliers detected")\n            else:\n                print(f"   {col}: No outliers detected")\n    \n    def split_data(self, test_size=0.2, random_state=None) -> tuple:\n        \"\"\"Split data into train and test sets\"\"\"\n        print("\n" + "="*60)\n        print("STEP 4: TRAIN-TEST SPLIT")\n        print("="*60)\n        \n        if random_state is None:\n            random_state = self.seed\n        \n        feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n        X = self.df[feature_cols]\n        y = self.df['species']\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            X, y, test_size=test_size, random_state=random_state, stratify=y\n        )\n        \n        print(f"Data split completed:")\n        print(f"  Training set: {len(self.X_train)} samples")\n        print(f"  Test set: {len(self.X_test)} samples")\n        print(f"  Test ratio: {test_size:.1%}")\n        \n        # Verify stratification\n        print(f"\nClass distribution in splits:")\n        print(f"  Train: {pd.Series(self.y_train).value_counts().sort_index().to_dict()}")\n        print(f"  Test:  {pd.Series(self.y_test).value_counts().sort_index().to_dict()}")\n        \n        return self.X_train, self.X_test, self.y_train, self.y_test\n    \n    def save_preprocessed_data(self) -> None:\n        \"\"\"Save preprocessed data to CSV\"\"\"\n        self.df.to_csv('preprocessed_iris.csv', index=False)\n        print("\n✓ Preprocessed data saved to 'preprocessed_iris.csv'")\n    \n    def run_complete_preprocessing(self) -> tuple:\n        \"\"\"Execute complete preprocessing pipeline\"\"\"\n        print("\n" + "🔬"*30)\n        print("IRIS DATASET - PREPROCESSING PIPELINE")\n        print("🔬"*30)\n        \n        # Load data\n        self.load_data()\n        \n        # Preprocess\n        self.preprocess_data()\n        \n        # Explore\n        self.explore_data()\n        \n        # Split\n        X_train, X_test, y_train, y_test = self.split_data()\n        \n        # Save\n        self.save_preprocessed_data()\n        \n        print("\n" + "="*60)\n        print("✅ PREPROCESSING PIPELINE COMPLETE!")\n        print("="*60)\n        print("\nGenerated files:")\n        print("  - preprocessed_iris.csv")\n        print("  - iris_pairplot.png")\n        print("  - iris_correlation.png")\n        print("  - iris_boxplots.png")\n        \n        return X_train, X_test, y_train, y_test\n\ndef main():\n    \"\"\"Main execution function\"\"\"\n    # Initialize preprocessor\n    preprocessor = IrisPreprocessor(use_synthetic=False, seed=42)\n    \n    # Run complete analysis\n    X_train, X_test, y_train, y_test = preprocessor.run_complete_preprocessing()\n    \n    print("\n📊 Data ready for machine learning tasks!")\n\nif __name__ == "__main__":\n    main()"