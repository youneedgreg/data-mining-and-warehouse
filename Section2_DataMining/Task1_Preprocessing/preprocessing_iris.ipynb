{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing and Exploration for Iris Dataset\n",
        "## Section 2, Task 1: Data Preprocessing and Exploration (15 Marks)\n",
        "\n",
        "This notebook demonstrates comprehensive data preprocessing techniques using the famous Iris dataset. We'll cover:\n",
        "- **Data Loading**: Load from sklearn or generate synthetic data\n",
        "- **Data Cleaning**: Handle missing values and outliers\n",
        "- **Feature Engineering**: Normalization and encoding\n",
        "- **Exploratory Data Analysis**: Statistics and visualizations\n",
        "- **Data Splitting**: Prepare for machine learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IrisPreprocessor Class\n",
        "\n",
        "A comprehensive class for handling all aspects of Iris dataset preprocessing and exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IrisPreprocessor:\n",
        "    \"\"\"Preprocessing and Exploration for Iris Dataset\"\"\"\n",
        "    \n",
        "    def __init__(self, use_synthetic=False, seed=42):\n",
        "        self.seed = seed\n",
        "        self.use_synthetic = use_synthetic\n",
        "        self.df = None\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Synthetic Data Generation\n",
        "\n",
        "Generate synthetic data that mimics the Iris dataset characteristics. This is useful for:\n",
        "- Testing preprocessing pipelines\n",
        "- Demonstrating handling of missing values\n",
        "- Creating larger datasets for experimentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def generate_synthetic_iris(self) -> pd.DataFrame:\n",
        "        \"\"\"Generate synthetic data mimicking Iris dataset\"\"\"\n",
        "        print(\"Generating synthetic Iris-like dataset...\")\n",
        "        \n",
        "        np.random.seed(self.seed)\n",
        "        \n",
        "        # Generate 150 samples (50 per class) with 4 features\n",
        "        n_samples_per_class = 50\n",
        "        \n",
        "        # Class 0 (Setosa-like): smaller measurements\n",
        "        class_0 = np.random.normal(loc=[5.0, 3.4, 1.5, 0.2], \n",
        "                                  scale=[0.35, 0.38, 0.17, 0.10], \n",
        "                                  size=(n_samples_per_class, 4))\n",
        "        \n",
        "        # Class 1 (Versicolor-like): medium measurements\n",
        "        class_1 = np.random.normal(loc=[5.9, 2.8, 4.3, 1.3], \n",
        "                                  scale=[0.51, 0.31, 0.47, 0.20], \n",
        "                                  size=(n_samples_per_class, 4))\n",
        "        \n",
        "        # Class 2 (Virginica-like): larger measurements\n",
        "        class_2 = np.random.normal(loc=[6.5, 3.0, 5.5, 2.0], \n",
        "                                  scale=[0.63, 0.32, 0.55, 0.27], \n",
        "                                  size=(n_samples_per_class, 4))\n",
        "        \n",
        "        # Combine all classes\n",
        "        X = np.vstack([class_0, class_1, class_2])\n",
        "        y = np.array([0]*n_samples_per_class + [1]*n_samples_per_class + [2]*n_samples_per_class)\n",
        "        \n",
        "        # Create DataFrame\n",
        "        df = pd.DataFrame(X, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
        "        df['species'] = y\n",
        "        df['species_name'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "        \n",
        "        # Add some missing values for demonstration (2% of data)\n",
        "        n_missing = int(0.02 * len(df) * 4)\n",
        "        for _ in range(n_missing):\n",
        "            row = np.random.randint(0, len(df))\n",
        "            col = np.random.choice(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
        "            df.loc[row, col] = np.nan\n",
        "        \n",
        "        print(f\"Generated {len(df)} samples with {df.isnull().sum().sum()} missing values\")\n",
        "        \n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 1: Data Loading\n",
        "\n",
        "Load the Iris dataset from sklearn or generate synthetic data for demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def load_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Load Iris dataset or generate synthetic data\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 1: LOADING DATA\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        if self.use_synthetic:\n",
        "            self.df = self.generate_synthetic_iris()\n",
        "        else:\n",
        "            # Load from sklearn\n",
        "            iris = load_iris()\n",
        "            self.df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "            self.df['species'] = iris.target\n",
        "            self.df['species_name'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
        "            \n",
        "            # Clean column names\n",
        "            self.df.columns = [col.replace(' (cm)', '').replace(' ', '_') for col in self.df.columns]\n",
        "        \n",
        "        print(f\"Dataset loaded successfully!\")\n",
        "        print(f\"Shape: {self.df.shape}\")\n",
        "        print(f\"Columns: {list(self.df.columns)}\")\n",
        "        \n",
        "        return self.df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 2: Data Preprocessing\n",
        "\n",
        "Clean and transform the data through several key steps:\n",
        "1. **Missing Value Handling**: Detect and impute missing values\n",
        "2. **Feature Normalization**: Scale features to [0,1] range using Min-Max scaling\n",
        "3. **Label Encoding**: Convert categorical species names to numerical labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def preprocess_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Preprocess the data: handle missing values, normalize, encode\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 2: PREPROCESSING\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # 1. Check for missing values\n",
        "        print(\"\\n1. Checking for missing values:\")\n",
        "        missing = self.df.isnull().sum()\n",
        "        print(missing[missing > 0] if missing.sum() > 0 else \"No missing values found\")\n",
        "        \n",
        "        # 2. Handle missing values (if any)\n",
        "        if self.df.isnull().sum().sum() > 0:\n",
        "            print(\"\\n2. Handling missing values with mean imputation...\")\n",
        "            numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
        "            for col in numeric_cols:\n",
        "                if self.df[col].isnull().sum() > 0:\n",
        "                    mean_val = self.df[col].mean()\n",
        "                    self.df[col].fillna(mean_val, inplace=True)\n",
        "                    print(f\"   Filled {col} with mean: {mean_val:.2f}\")\n",
        "        \n",
        "        # 3. Normalize features using Min-Max scaling\n",
        "        print(\"\\n3. Normalizing features using Min-Max scaling...\")\n",
        "        scaler = MinMaxScaler()\n",
        "        feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "        self.df[feature_cols] = scaler.fit_transform(self.df[feature_cols])\n",
        "        print(\"   Features normalized to range [0, 1]\")\n",
        "        \n",
        "        # 4. Encode the class label\n",
        "        print(\"\\n4. Encoding class labels...\")\n",
        "        le = LabelEncoder()\n",
        "        self.df['species_encoded'] = le.fit_transform(self.df['species_name'])\n",
        "        print(f\"   Classes encoded: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
        "        \n",
        "        return self.df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 3: Exploratory Data Analysis\n",
        "\n",
        "Comprehensive exploration of the dataset including:\n",
        "- **Summary Statistics**: Mean, std, min, max for all features\n",
        "- **Class Distribution**: Balance of species in the dataset\n",
        "- **Correlation Analysis**: Relationships between features\n",
        "- **Data Visualizations**: Pairplots, heatmaps, and boxplots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def explore_data(self) -> None:\n",
        "        \"\"\"Explore data with statistics and visualizations\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 3: DATA EXPLORATION\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # 1. Summary statistics\n",
        "        print(\"\\n1. Summary Statistics:\")\n",
        "        print(self.df.describe().round(3))\n",
        "        \n",
        "        # 2. Class distribution\n",
        "        print(\"\\n2. Class Distribution:\")\n",
        "        print(self.df['species_name'].value_counts())\n",
        "        \n",
        "        # 3. Correlation matrix\n",
        "        print(\"\\n3. Correlation Matrix:\")\n",
        "        feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "        corr_matrix = self.df[feature_cols].corr()\n",
        "        print(corr_matrix.round(3))\n",
        "        \n",
        "        # Create visualizations\n",
        "        self.create_visualizations()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Visualizations\n",
        "\n",
        "Create comprehensive visualizations to understand the data patterns and relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def create_visualizations(self) -> None:\n",
        "        \"\"\"Create and save visualization plots\"\"\"\n",
        "        print(\"\\n4. Creating Visualizations...\")\n",
        "        \n",
        "        feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "        \n",
        "        # Set style\n",
        "        sns.set_style(\"whitegrid\")\n",
        "        \n",
        "        # 1. Pairplot\n",
        "        fig = plt.figure(figsize=(12, 10))\n",
        "        pairplot_data = self.df[feature_cols + ['species_name']].copy()\n",
        "        g = sns.pairplot(pairplot_data, hue='species_name', palette='Set1', diag_kind='kde')\n",
        "        g.fig.suptitle('Iris Dataset - Pairplot', y=1.02, fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('iris_pairplot.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(\"   ✓ Pairplot saved as 'iris_pairplot.png'\")\n",
        "        \n",
        "        # 2. Correlation Heatmap\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        corr_matrix = self.df[feature_cols].corr()\n",
        "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "                   square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "        plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('iris_correlation.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(\"   ✓ Correlation heatmap saved as 'iris_correlation.png'\")\n",
        "        \n",
        "        # 3. Boxplots for outlier detection\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "        for idx, col in enumerate(feature_cols):\n",
        "            ax = axes[idx // 2, idx % 2]\n",
        "            self.df.boxplot(column=col, by='species_name', ax=ax)\n",
        "            ax.set_title(f'{col.replace(\"_\", \" \").title()} by Species')\n",
        "            ax.set_xlabel('Species')\n",
        "            ax.set_ylabel(col.replace(\"_\", \" \").title())\n",
        "            ax.get_figure().suptitle('')\n",
        "        \n",
        "        plt.suptitle('Boxplots for Outlier Detection', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('iris_boxplots.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(\"   ✓ Boxplots saved as 'iris_boxplots.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Outlier Detection\n",
        "\n",
        "Identify outliers using the Interquartile Range (IQR) method for each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "        # Identify outliers\n",
        "        print(\"\\n5. Outlier Detection (using IQR method):\")\n",
        "        for col in feature_cols:\n",
        "            Q1 = self.df[col].quantile(0.25)\n",
        "            Q3 = self.df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            outliers = self.df[(self.df[col] < Q1 - 1.5*IQR) | (self.df[col] > Q3 + 1.5*IQR)]\n",
        "            if len(outliers) > 0:\n",
        "                print(f\"   {col}: {len(outliers)} outliers detected\")\n",
        "            else:\n",
        "                print(f\"   {col}: No outliers detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 4: Train-Test Split\n",
        "\n",
        "Split the data into training and testing sets with proper stratification to maintain class balance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def split_data(self, test_size=0.2, random_state=None) -> tuple:\n",
        "        \"\"\"Split data into train and test sets\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 4: TRAIN-TEST SPLIT\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        if random_state is None:\n",
        "            random_state = self.seed\n",
        "        \n",
        "        feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "        X = self.df[feature_cols]\n",
        "        y = self.df['species']\n",
        "        \n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "        )\n",
        "        \n",
        "        print(f\"Data split completed:\")\n",
        "        print(f\"  Training set: {len(self.X_train)} samples\")\n",
        "        print(f\"  Test set: {len(self.X_test)} samples\")\n",
        "        print(f\"  Test ratio: {test_size:.1%}\")\n",
        "        \n",
        "        # Verify stratification\n",
        "        print(f\"\\nClass distribution in splits:\")\n",
        "        print(f\"  Train: {pd.Series(self.y_train).value_counts().sort_index().to_dict()}\")\n",
        "        print(f\"  Test:  {pd.Series(self.y_test).value_counts().sort_index().to_dict()}\")\n",
        "        \n",
        "        return self.X_train, self.X_test, self.y_train, self.y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Preprocessed Data\n",
        "\n",
        "Save the cleaned and preprocessed data to CSV for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def save_preprocessed_data(self) -> None:\n",
        "        \"\"\"Save preprocessed data to CSV\"\"\"\n",
        "        self.df.to_csv('preprocessed_iris.csv', index=False)\n",
        "        print(\"\\n✓ Preprocessed data saved to 'preprocessed_iris.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Preprocessing Pipeline\n",
        "\n",
        "Execute the entire preprocessing workflow in sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def run_complete_preprocessing(self) -> tuple:\n",
        "        \"\"\"Execute complete preprocessing pipeline\"\"\"\n",
        "        print(\"\\n\" + \"🔬\"*30)\n",
        "        print(\"IRIS DATASET - PREPROCESSING PIPELINE\")\n",
        "        print(\"🔬\"*30)\n",
        "        \n",
        "        # Load data\n",
        "        self.load_data()\n",
        "        \n",
        "        # Preprocess\n",
        "        self.preprocess_data()\n",
        "        \n",
        "        # Explore\n",
        "        self.explore_data()\n",
        "        \n",
        "        # Split\n",
        "        X_train, X_test, y_train, y_test = self.split_data()\n",
        "        \n",
        "        # Save\n",
        "        self.save_preprocessed_data()\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"✅ PREPROCESSING PIPELINE COMPLETE!\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"\\nGenerated files:\")\n",
        "        print(\"  - preprocessed_iris.csv\")\n",
        "        print(\"  - iris_pairplot.png\")\n",
        "        print(\"  - iris_correlation.png\")\n",
        "        print(\"  - iris_boxplots.png\")\n",
        "        \n",
        "        return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Execution Function\n",
        "\n",
        "Main function to initialize and run the complete preprocessing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    # Initialize preprocessor\n",
        "    preprocessor = IrisPreprocessor(use_synthetic=False, seed=42)\n",
        "    \n",
        "    # Run complete preprocessing pipeline\n",
        "    X_train, X_test, y_train, y_test = preprocessor.run_complete_preprocessing()\n",
        "    \n",
        "    print(\"\\n📊 Data ready for machine learning tasks!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute the Complete Pipeline\n",
        "\n",
        "Run the main function to execute the entire preprocessing workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Usage Examples\n",
        "\n",
        "You can also run individual steps for experimentation and learning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create preprocessor instance\n",
        "preprocessor = IrisPreprocessor(use_synthetic=False, seed=42)\n",
        "\n",
        "# Step 1: Load data\n",
        "df = preprocessor.load_data()\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Preprocess data\n",
        "preprocessed_df = preprocessor.preprocess_data()\n",
        "print(\"\\nPreprocessed data info:\")\n",
        "print(preprocessed_df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Explore data (creates visualizations)\n",
        "preprocessor.explore_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Split data\n",
        "X_train, X_test, y_train, y_test = preprocessor.split_data()\n",
        "print(f\"Training features shape: {X_train.shape}\")\n",
        "print(f\"Test features shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment with Synthetic Data\n",
        "\n",
        "Try using synthetic data to see how the pipeline handles missing values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create preprocessor with synthetic data\n",
        "synthetic_preprocessor = IrisPreprocessor(use_synthetic=True, seed=42)\n",
        "\n",
        "# Run complete pipeline with synthetic data\n",
        "X_train_syn, X_test_syn, y_train_syn, y_test_syn = synthetic_preprocessor.run_complete_preprocessing()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
