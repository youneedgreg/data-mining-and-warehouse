{
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# ETL Process Implementation for Retail Data Warehouse\n",
          "## Task 2: ETL Process Implementation (20 Marks)\n",
          "\n",
          "This notebook implements a complete ETL (Extract, Transform, Load) pipeline for building a retail data warehouse. The process includes:\n",
          "- **Extract**: Generate synthetic retail data or load from CSV\n",
          "- **Transform**: Clean data, handle outliers, create dimension tables\n",
          "- **Load**: Insert data into SQLite database with star schema"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "import pandas as pd\n",
          "import numpy as np\n",
          "import sqlite3\n",
          "from datetime import datetime, timedelta\n",
          "import random\n",
          "import logging\n",
          "from typing import Tuple, Dict"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Configure Logging\n",
          "\n",
          "Set up logging to track the ETL process progress and any issues that occur."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Configure logging\n",
          "logging.basicConfig(\n",
          "    level=logging.INFO,\n",
          "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
          ")\n",
          "logger = logging.getLogger(__name__)"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## RetailETL Class\n",
          "\n",
          "The main ETL pipeline class that handles all three phases of the ETL process."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "class RetailETL:\n",
          "    \"\"\"ETL Pipeline for Retail Data Warehouse\"\"\"\n",
          "    \n",
          "    def __init__(self, db_path='retail_dw.db'):\n",
          "        self.db_path = db_path\n",
          "        self.conn = None\n",
          "        self.raw_data = None\n",
          "        self.transformed_data = {}"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "### Data Generation\n",
          "\n",
          "Generate synthetic retail data for demonstration purposes. This creates realistic transaction data with various products, customers, and time periods."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "    def generate_synthetic_data(self, num_rows=1000, seed=42) -> pd.DataFrame:\n",
          "        \"\"\"Generate synthetic retail data\"\"\"\n",
          "        np.random.seed(seed)\n",
          "        random.seed(seed)\n",
          "        \n",
          "        logger.info(f\"Generating {num_rows} rows of synthetic retail data...\")\n",
          "        \n",
          "        # Define data pools\n",
          "        products = [\n",
          "            ('ELEC001', 'Laptop', 'Electronics', 899.99),\n",
          "            ('ELEC002', 'Smartphone', 'Electronics', 599.99),\n",
          "            ('ELEC003', 'Tablet', 'Electronics', 399.99),\n",
          "            ('ELEC004', 'Headphones', 'Electronics', 149.99),\n",
          "            ('ELEC005', 'Smart Watch', 'Electronics', 299.99),\n",
          "            ('CLTH001', 'T-Shirt', 'Clothing', 29.99),\n",
          "            ('CLTH002', 'Jeans', 'Clothing', 79.99),\n",
          "            ('CLTH003', 'Jacket', 'Clothing', 129.99),\n",
          "            ('CLTH004', 'Shoes', 'Clothing', 89.99),\n",
          "            ('CLTH005', 'Hat', 'Clothing', 24.99),\n",
          "            ('HOME001', 'Coffee Maker', 'Home', 79.99),\n",
          "            ('HOME002', 'Blender', 'Home', 49.99),\n",
          "            ('HOME003', 'Toaster', 'Home', 34.99),\n",
          "            ('BOOK001', 'Novel', 'Books', 14.99),\n",
          "            ('BOOK002', 'Textbook', 'Books', 89.99),\n",
          "        ]\n",
          "        \n",
          "        countries = ['USA', 'UK', 'Germany', 'France', 'Canada', 'Australia', 'Japan', 'Brazil']\n",
          "        customer_ids = [f'CUST{str(i).zfill(4)}' for i in range(1, 101)]\n",
          "        \n",
          "        # Generate dates over past 2 years\n",
          "        end_date = datetime.now()\n",
          "        start_date = end_date - timedelta(days=730)\n",
          "        \n",
          "        data = []\n",
          "        for i in range(num_rows):\n",
          "            # Random date\n",
          "            days_offset = random.randint(0, 730)\n",
          "            invoice_date = start_date + timedelta(days=days_offset)\n",
          "            \n",
          "            # Random product\n",
          "            product = random.choice(products)\n",
          "            \n",
          "            # Random quantity (with some negative for returns)\n",
          "            quantity = random.choices(\n",
          "                [random.randint(1, 10), random.randint(-3, -1)],\n",
          "                weights=[0.95, 0.05]\n",
          "            )[0]\n",
          "            \n",
          "            # Price variation (Â±10% from base price)\n",
          "            unit_price = product[3] * random.uniform(0.9, 1.1)\n",
          "            \n",
          "            data.append({\n",
          "                'InvoiceNo': f'INV{str(i+1).zfill(6)}',\n",
          "                'StockCode': product[0],\n",
          "                'Description': product[1],\n",
          "                'Category': product[2],\n",
          "                'Quantity': quantity,\n",
          "                'InvoiceDate': invoice_date,\n",
          "                'UnitPrice': round(unit_price, 2),\n",
          "                'CustomerID': random.choice(customer_ids),\n",
          "                'Country': random.choice(countries)\n",
          "            })\n",
          "        \n",
          "        df = pd.DataFrame(data)\n",
          "        \n",
          "        # Add some missing values for demonstration\n",
          "        missing_indices = np.random.choice(df.index, size=int(0.02 * len(df)), replace=False)\n",
          "        df.loc[missing_indices, 'CustomerID'] = np.nan\n",
          "        \n",
          "        logger.info(f\"Generated {len(df)} rows of synthetic data\")\n",
          "        return df"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## EXTRACT Phase\n",
          "\n",
          "The extract phase loads data from external sources. It can either generate synthetic data or load from a CSV file."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "    def extract(self, data_source=None) -> pd.DataFrame:\n",
          "        \"\"\"Extract phase: Load data from CSV or use generated data\"\"\"\n",
          "        logger.info(\"Starting EXTRACT phase...\")\n",
          "        \n",
          "        if data_source is None:\n",
          "            # Generate synthetic data\n",
          "            self.raw_data = self.generate_synthetic_data()\n",
          "        else:\n",
          "            # Load from CSV\n",
          "            self.raw_data = pd.read_csv(data_source)\n",
          "            logger.info(f\"Loaded {len(self.raw_data)} rows from {data_source}\")\n",
          "        \n",
          "        # Convert InvoiceDate to datetime\n",
          "        self.raw_data['InvoiceDate'] = pd.to_datetime(self.raw_data['InvoiceDate'])\n",
          "        \n",
          "        logger.info(f\"Extracted {len(self.raw_data)} total rows\")\n",
          "        logger.info(f\"Columns: {list(self.raw_data.columns)}\")\n",
          "        logger.info(f\"Missing values:\\n{self.raw_data.isnull().sum()}\")\n",
          "        \n",
          "        return self.raw_data"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## TRANSFORM Phase\n",
          "\n",
          "The transform phase is the core of the ETL process. It:\n",
          "1. **Cleans the data**: Handle missing values and outliers\n",
          "2. **Calculates metrics**: Derive new fields like TotalSales\n",
          "3. **Creates dimensions**: Build dimension tables for the star schema\n",
          "4. **Builds fact table**: Create the central fact table with foreign keys"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "    def transform(self) -> Dict[str, pd.DataFrame]:\n",
          "        \"\"\"Transform phase: Clean, calculate, and prepare dimension tables\"\"\"\n",
          "        logger.info(\"Starting TRANSFORM phase...\")\n",
          "        \n",
          "        df = self.raw_data.copy()\n",
          "        initial_rows = len(df)\n",
          "        \n",
          "        # 1. Handle missing values\n",
          "        logger.info(\"Handling missing values...\")\n",
          "        df['CustomerID'] = df['CustomerID'].fillna('UNKNOWN')\n",
          "        \n",
          "        # 2. Remove outliers (negative quantity and zero/negative prices)\n",
          "        logger.info(\"Removing outliers...\")\n",
          "        df = df[df['Quantity'] > 0]\n",
          "        df = df[df['UnitPrice'] > 0]\n",
          "        outliers_removed = initial_rows - len(df)\n",
          "        logger.info(f\"Removed {outliers_removed} rows with outliers\")\n",
          "        \n",
          "        # 3. Calculate TotalSales\n",
          "        df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
          "        \n",
          "        # 4. Filter for last year (from August 12, 2025)\n",
          "        current_date = datetime(2025, 8, 12)\n",
          "        one_year_ago = current_date - timedelta(days=365)\n",
          "        df = df[df['InvoiceDate'] >= one_year_ago]\n",
          "        logger.info(f\"Filtered to {len(df)} rows for last year\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "### Create Dimension Tables\n",
          "\n",
          "Build the dimension tables that form the star schema structure."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "        # 5. Create dimension tables\n",
          "        \n",
          "        # Time Dimension\n",
          "        time_dim = pd.DataFrame()\n",
          "        time_dim['time_id'] = range(1, len(df) + 1)\n",
          "        time_dim['date'] = df['InvoiceDate'].values\n",
          "        time_dim['day'] = pd.to_datetime(time_dim['date']).dt.day\n",
          "        time_dim['month'] = pd.to_datetime(time_dim['date']).dt.month\n",
          "        time_dim['quarter'] = pd.to_datetime(time_dim['date']).dt.quarter\n",
          "        time_dim['year'] = pd.to_datetime(time_dim['date']).dt.year\n",
          "        time_dim['day_of_week'] = pd.to_datetime(time_dim['date']).dt.day_name()\n",
          "        time_dim['month_name'] = pd.to_datetime(time_dim['date']).dt.month_name()\n",
          "        time_dim['is_weekend'] = pd.to_datetime(time_dim['date']).dt.dayofweek >= 5\n",
          "        time_dim = time_dim.drop_duplicates(subset=['date'])\n",
          "        \n",
          "        # Customer Dimension\n",
          "        customer_dim = df[['CustomerID', 'Country']].drop_duplicates()\n",
          "        customer_dim = customer_dim.reset_index(drop=True)\n",
          "        customer_dim['customer_id'] = range(1, len(customer_dim) + 1)\n",
          "        customer_dim['customer_name'] = customer_dim['CustomerID'].apply(\n",
          "            lambda x: f\"Customer {x}\" if x != 'UNKNOWN' else 'Unknown Customer'\n",
          "        )\n",
          "        customer_dim['customer_segment'] = np.random.choice(\n",
          "            ['Premium', 'Standard', 'Basic'], \n",
          "            size=len(customer_dim)\n",
          "        )\n",
          "        \n",
          "        # Product Dimension\n",
          "        product_dim = df[['StockCode', 'Description', 'Category']].drop_duplicates()\n",
          "        product_dim = product_dim.reset_index(drop=True)\n",
          "        product_dim['product_id'] = range(1, len(product_dim) + 1)\n",
          "        product_dim['product_code'] = product_dim['StockCode']\n",
          "        product_dim['product_name'] = product_dim['Description']"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "### Create Fact Table\n",
          "\n",
          "Build the central fact table with foreign keys linking to all dimension tables."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "        # Create fact table with foreign keys\n",
          "        fact_table = df.copy()\n",
          "        \n",
          "        # Map to dimension IDs\n",
          "        fact_table = fact_table.merge(\n",
          "            time_dim[['date', 'time_id']], \n",
          "            left_on='InvoiceDate', \n",
          "            right_on='date',\n",
          "            how='left'\n",
          "        )\n",
          "        fact_table = fact_table.merge(\n",
          "            customer_dim[['CustomerID', 'customer_id']], \n",
          "            on='CustomerID',\n",
          "            how='left'\n",
          "        )\n",
          "        fact_table = fact_table.merge(\n",
          "            product_dim[['StockCode', 'product_id']], \n",
          "            on='StockCode',\n",
          "            how='left'\n",
          "        )\n",
          "        \n",
          "        # Sales Fact table\n",
          "        sales_fact = pd.DataFrame({\n",
          "            'time_id': fact_table['time_id'],\n",
          "            'customer_id': fact_table['customer_id'],\n",
          "            'product_id': fact_table['product_id'],\n",
          "            'invoice_no': fact_table['InvoiceNo'],\n",
          "            'quantity': fact_table['Quantity'],\n",
          "            'unit_price': fact_table['UnitPrice'],\n",
          "            'total_amount': fact_table['TotalSales']\n",
          "        })\n",
          "        \n",
          "        self.transformed_data = {\n",
          "            'sales_fact': sales_fact,\n",
          "            'time_dim': time_dim,\n",
          "            'customer_dim': customer_dim,\n",
          "            'product_dim': product_dim\n",
          "        }\n",
          "        \n",
          "        logger.info(f\"Transformation complete:\")\n",
          "        for table_name, table_df in self.transformed_data.items():\n",
          "            logger.info(f\"  {table_name}: {len(table_df)} rows\")\n",
          "        \n",
          "        return self.transformed_data"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## LOAD Phase\n",
          "\n",
          "The load phase creates the database schema and inserts all transformed data into SQLite tables. This implements a star schema with:\n",
          "- **Fact Table**: SalesFact (central table with measures)\n",
          "- **Dimension Tables**: TimeDim, CustomerDim, ProductDim"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "    def load(self) -> None:\n",
          "        \"\"\"Load phase: Insert data into SQLite database\"\"\"\n",
          "        logger.info(\"Starting LOAD phase...\")\n",
          "        \n",
          "        try:\n",
          "            self.conn = sqlite3.connect(self.db_path)\n",
          "            \n",
          "            # Create tables (using simplified schema)\n",
          "            self.conn.execute('''\n",
          "                CREATE TABLE IF NOT EXISTS TimeDim (\n",
          "                    time_id INTEGER PRIMARY KEY,\n",
          "                    date DATE,\n",
          "                    day INTEGER,\n",
          "                    month INTEGER,\n",
          "                    quarter INTEGER,\n",
          "                    year INTEGER,\n",
          "                    day_of_week TEXT,\n",
          "                    month_name TEXT,\n",
          "                    is_weekend BOOLEAN\n",
          "                )\n",
          "            ''')\n",
          "            \n",
          "            self.conn.execute('''\n",
          "                CREATE TABLE IF NOT EXISTS CustomerDim (\n",
          "                    customer_id INTEGER PRIMARY KEY,\n",
          "                    CustomerID TEXT,\n",
          "                    customer_name TEXT,\n",
          "                    Country TEXT,\n",
          "                    customer_segment TEXT\n",
          "                )\n",
          "            ''')\n",
          "            \n",
          "            self.conn.execute('''\n",
          "                CREATE TABLE IF NOT EXISTS ProductDim (\n",
          "                    product_id INTEGER PRIMARY KEY,\n",
          "                    product_code TEXT,\n",
          "                    product_name TEXT,\n",
          "                    category TEXT\n",
          "                )\n",
          "            ''')\n",
          "            \n",
          "            self.conn.execute('''\n",
          "                CREATE TABLE IF NOT EXISTS SalesFact (\n",
          "                    sale_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
          "                    time_id INTEGER,\n",
          "                    customer_id INTEGER,\n",
          "                    product_id INTEGER,\n",
          "                    invoice_no TEXT,\n",
          "                    quantity INTEGER,\n",
          "                    unit_price REAL,\n",
          "                    total_amount REAL,\n",
          "                    FOREIGN KEY (time_id) REFERENCES TimeDim(time_id),\n",
          "                    FOREIGN KEY (customer_id) REFERENCES CustomerDim(customer_id),\n",
          "                    FOREIGN KEY (product_id) REFERENCES ProductDim(product_id)\n",
          "                )\n",
          "            ''')"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "### Insert Data into Tables\n",
          "\n",
          "Load all transformed data into the database tables and verify the load was successful."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "            # Load data into tables\n",
          "            self.transformed_data['time_dim'].to_sql(\n",
          "                'TimeDim', self.conn, if_exists='replace', index=False\n",
          "            )\n",
          "            self.transformed_data['customer_dim'].to_sql(\n",
          "                'CustomerDim', self.conn, if_exists='replace', index=False\n",
          "            )\n",
          "            self.transformed_data['product_dim'].to_sql(\n",
          "                'ProductDim', self.conn, if_exists='replace', index=False\n",
          "            )\n",
          "            self.transformed_data['sales_fact'].to_sql(\n",
          "                'SalesFact', self.conn, if_exists='replace', index=False\n",
          "            )\n",
          "            \n",
          "            self.conn.commit()\n",
          "            logger.info(f\"Data successfully loaded to {self.db_path}\")\n",
          "            \n",
          "            # Verify loaded data\n",
          "            for table in ['TimeDim', 'CustomerDim', 'ProductDim', 'SalesFact']:\n",
          "                count = self.conn.execute(f\"SELECT COUNT(*) FROM {table}\").fetchone()[0]\n",
          "                logger.info(f\"  {table}: {count} rows loaded\")\n",
          "                \n",
          "        except Exception as e:\n",
          "            logger.error(f\"Error during load phase: {e}\")\n",
          "            if self.conn:\n",
          "                self.conn.rollback()\n",
          "            raise\n",
          "        finally:\n",
          "            if self.conn:\n",
          "                self.conn.close()"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Complete ETL Pipeline\n",
          "\n",
          "Execute the entire ETL process in sequence and return statistics about the processing."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "    def run_etl(self) -> Tuple[int, int, int]:\n",
          "        \"\"\"Execute the complete ETL pipeline\"\"\"\n",
          "        logger.info(\"=\"*50)\n",
          "        logger.info(\"Starting ETL Pipeline\")\n",
          "        logger.info(\"=\"*50)\n",
          "        \n",
          "        # Extract\n",
          "        extracted_data = self.extract()\n",
          "        extracted_rows = len(extracted_data)\n",
          "        \n",
          "        # Transform\n",
          "        transformed_data = self.transform()\n",
          "        transformed_rows = len(transformed_data['sales_fact'])\n",
          "        \n",
          "        # Load\n",
          "        self.load()\n",
          "        loaded_rows = transformed_rows\n",
          "        \n",
          "        logger.info(\"=\"*50)\n",
          "        logger.info(\"ETL Pipeline Complete\")\n",
          "        logger.info(f\"Rows processed - Extract: {extracted_rows}, Transform: {transformed_rows}, Load: {loaded_rows}\")\n",
          "        logger.info(\"=\"*50)\n",
          "        \n",
          "        return extracted_rows, transformed_rows, loaded_rows"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Main Execution Function\n",
          "\n",
          "Main function to run the complete ETL pipeline and display results."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "def main():\n",
          "    \"\"\"Main execution function\"\"\"\n",
          "    etl = RetailETL('retail_dw.db')\n",
          "    \n",
          "    # Run the ETL pipeline\n",
          "    extracted, transformed, loaded = etl.run_etl()\n",
          "    \n",
          "    print(f\"ETL Process Complete!\")\n",
          "    print(f\"Statistics:\")\n",
          "    print(f\"  - Extracted: {extracted} rows\")\n",
          "    print(f\"  - Transformed: {transformed} rows\")\n",
          "    print(f\"  - Loaded: {loaded} rows\")\n",
          "    print(f\"  - Database: retail_dw.db\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Execute the ETL Pipeline\n",
          "\n",
          "Run the complete ETL process to build the data warehouse."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "if __name__ == \"__main__\":\n",
          "    main()"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Interactive ETL Execution\n",
          "\n",
          "You can also run the ETL phases individually for testing and exploration:"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Create ETL instance\n",
          "etl = RetailETL('retail_dw.db')\n",
          "\n",
          "# Run individual phases\n",
          "# 1. Extract phase\n",
          "raw_data = etl.extract()\n",
          "print(f\"Extracted {len(raw_data)} rows\")\n",
          "print(raw_data.head())"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# 2. Transform phase\n",
          "transformed_tables = etl.transform()\n",
          "print(\"\\nTransformed tables:\")\n",
          "for table_name, df in transformed_tables.items():\n",
          "    print(f\"{table_name}: {len(df)} rows\")\n",
          "    print(df.head())\n",
          "    print()"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# 3. Load phase\n",
          "etl.load()\n",
          "print(\"Data loaded to retail_dw.db\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Verify Database Creation\n",
          "\n",
          "Check that the database was created successfully and examine the schema:"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Connect to database and verify tables\n",
          "conn = sqlite3.connect('retail_dw.db')\n",
          "\n",
          "# List all tables\n",
          "tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
          "print(\"Tables in database:\")\n",
          "print(tables)\n",
          "\n",
          "# Check row counts\n",
          "for table in ['TimeDim', 'CustomerDim', 'ProductDim', 'SalesFact']:\n",
          "    count = pd.read_sql_query(f\"SELECT COUNT(*) as count FROM {table}\", conn)\n",
          "    print(f\"{table}: {count['count'][0]} rows\")\n",
          "\n",
          "conn.close()"
        ]
      }
    ],
    "metadata": {
      "kernelspec": {
        "display_name": "Python 3",
        "language": "python",
        "name": "python3"
      },
      "language_info": {
        "name": "python",
        "version": "3.8.0",
        "codemirror_mode": {
          "name": "ipython",
          "version": 3
        },
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "nbconvert_exporter": "python",
        "pygments_lexer": "ipython3"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 4
  }